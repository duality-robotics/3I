{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442ebd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import torchvision\n",
    "import os \n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import fiftyone.brain as fob\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5db568",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13219b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 281/281 [116.5ms elapsed, 0s remaining, 2.4K samples/s]  \n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "\n",
    "data_root2 = '../Data/Snickers_Real_Image/10/'\n",
    "data_root1 = '../Data/Snickers_Digital_Twin/10/'\n",
    "\n",
    "folder1 = os.listdir(data_root1)\n",
    "folder2 = os.listdir(data_root2)\n",
    "\n",
    "\n",
    "for count, file in enumerate(folder1):\n",
    "    if count >len(folder2):\n",
    "        break\n",
    "    sample = fo.Sample(filepath = os.path.join(data_root1,file))\n",
    "    label = 'Synthetic'\n",
    "    sample[\"ground_truth\"] = fo.Classification(label=label)\n",
    "    samples.append(sample)\n",
    "\n",
    "for count, file in enumerate(folder2):\n",
    "    sample = fo.Sample(filepath = os.path.join(data_root2,file))\n",
    "    label = 'Real'\n",
    "    sample[\"ground_truth\"] = fo.Classification(label=label)\n",
    "    samples.append(sample)\n",
    "\n",
    "# Create dataset\n",
    "dataset = fo.Dataset(\"my-classification-dataset\")\n",
    "dataset.add_samples(samples)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9305795f",
   "metadata": {},
   "source": [
    "## Image loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a042c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path: str) -> Image.Image:\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "    \n",
    "def get_image(path,transform):\n",
    "    \n",
    "    x = pil_loader(path)\n",
    "    x = transform(x)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26055cb5",
   "metadata": {},
   "source": [
    "## Calculate image normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5def041",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.Resize(size = (256,256)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "for count, file in enumerate(folder1):\n",
    "    im_file = os.path.join(data_root1, file)\n",
    "    images = get_image(im_file, train_transform)\n",
    "    if count==0:\n",
    "        image_stats = images.view(3,-1)\n",
    "    else:\n",
    "        image_stats = torch.cat((images.view(3,-1), image_stats), dim = 1)\n",
    "\n",
    "R = image_stats[0,:]\n",
    "G = image_stats[1,:]\n",
    "B = image_stats[2,:]\n",
    "\n",
    "R1 = R[R!=0].mean()\n",
    "G1 = G[G!=0].mean()\n",
    "B1 = B[B!=0].mean()\n",
    "Rstd1 = (R[R!=0].std())\n",
    "Gstd1 = (G[G!=0].std())\n",
    "Bstd1 = (B[B!=0].std())\n",
    "\n",
    "for count, file in enumerate(folder2):\n",
    "    im_file = os.path.join(data_root2, file)\n",
    "    images = get_image(im_file, train_transform)\n",
    "    if count==0:\n",
    "        image_stats = images.view(3,-1)\n",
    "    else:\n",
    "        image_stats = torch.cat((images.view(3,-1), image_stats), dim = 1)\n",
    "        \n",
    "R = image_stats[0,:]\n",
    "G = image_stats[1,:]\n",
    "B = image_stats[2,:]\n",
    "\n",
    "R2 = R[R!=0].mean()\n",
    "G2 = G[G!=0].mean()\n",
    "B2 = B[B!=0].mean()\n",
    "Rstd2 = (R[R!=0].std())\n",
    "Gstd2 = (G[G!=0].std())\n",
    "Bstd2 = (B[B!=0].std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bf80b",
   "metadata": {},
   "source": [
    "## Image Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ba039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_transform = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.Resize(size = (256,256)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[R1, G1, B1],\n",
    "                 std=[Rstd1, Gstd1, Bstd1]),\n",
    "    ## undo fasterrcnn normalization\n",
    "    torchvision.transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                 std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "    torchvision.transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                 std = [ 1., 1., 1. ]),\n",
    "])\n",
    "\n",
    "real_transform = torchvision.transforms.Compose([ \n",
    "    torchvision.transforms.Resize(size = (256,256)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[R2, G2, B2],\n",
    "             std=[Rstd2, Gstd2, Bstd2]),\n",
    "    ## undo fasterrcnn normalization\n",
    "    torchvision.transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                 std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "    torchvision.transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                 std = [ 1., 1., 1. ]),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4741ae",
   "metadata": {},
   "source": [
    "## Setup pre-trained model for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c85f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "\n",
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): \n",
    "        self.hook = m.register_forward_hook(self.hook_fn)\n",
    "        self.features = None\n",
    "    def hook_fn(self, module, input, output): \n",
    "        out = output.detach()#.cpu().numpy()\n",
    "        self.features = out.flatten()\n",
    "\n",
    "    def remove(self): \n",
    "        self.hook.remove()\n",
    "        \n",
    "hook = SaveFeatures(model.roi_heads.box_head.fc7)\n",
    "\n",
    "\n",
    "def model_embedding(model, images):\n",
    "    ## renset50\n",
    "    _ = model(images)\n",
    "    return hook.features[0:1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c9228",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ca1eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/famejia/anaconda3/envs/voxel51/lib/python3.6/site-packages/torch/functional.py:445: UserWarning:\n",
      "\n",
      "torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "n_features = 1024\n",
    "num_images = len(dataset.values('filepath'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    embedding = torch.zeros(num_images, n_features)\n",
    "    index_count = 0 \n",
    "    for index, (f, label_dict) in enumerate(zip(dataset.values('filepath'), dataset.values('ground_truth'))):\n",
    "        if 'Synthetic' in label_dict['label']:\n",
    "            transform = fake_transform\n",
    "        \n",
    "\n",
    "        elif 'Real'  in label_dict['label']:\n",
    "            transform = real_transform\n",
    "        images = get_image(f, transform)\n",
    "        images = images.unsqueeze(0).to(device)  \n",
    "        \n",
    "        out = model_embedding(model, images.to(device))\n",
    "        embedding[index,:] = (out)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f102d61",
   "metadata": {},
   "source": [
    "## umap Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b67910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "UMAP(dens_frac=0.0, dens_lambda=0.0, random_state=51, verbose=True)\n",
      "Construct fuzzy simplicial set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/famejia/anaconda3/envs/voxel51/lib/python3.6/site-packages/numba/np/ufunc/parallel.py:363: NumbaWarning:\n",
      "\n",
      "The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 20 11:39:58 2022 Finding Nearest Neighbors\n",
      "Fri May 20 11:40:00 2022 Finished Nearest Neighbor Search\n",
      "Fri May 20 11:40:02 2022 Construct embedding\n",
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Fri May 20 11:40:03 2022 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=embedding.detach().cpu().numpy(),\n",
    "    num_dims=2,\n",
    "    method=\"umap\",\n",
    "    brain_key=\"None\",\n",
    "    verbose=True,\n",
    "    seed=51,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1edba",
   "metadata": {},
   "source": [
    "## Voxel 51 visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1bf0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=true&handleId=0e1bdade-e2de-4a84-8eb2-8d8010f12bee\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f270446c940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb54b636f09479d80e26a4df8cb906e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'customdata': array(['6287e027ad35272bd64cb40e', '6287e027ad35272bd64cb40f',\n",
       "    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(view=dataset.view())\n",
    "plot = results.visualize(labels=\"ground_truth.label\")\n",
    "plot.show(height=720)\n",
    "session.plots.attach(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf8354",
   "metadata": {},
   "source": [
    "## Calculate data overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d87d3974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data overlap = 0.854103\n"
     ]
    }
   ],
   "source": [
    "y = np.array([int('Real' in d['label']) for d in dataset.values('ground_truth')])\n",
    "X = results.points\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)\n",
    "nearest_class = []\n",
    "\n",
    "## find nearest points to Synthetic data\n",
    "for data in X[y==0]:\n",
    "    distances, indices = nbrs.kneighbors(data.reshape(1, -1))\n",
    "    nearest_class.append(y[indices[0][1]])    \n",
    "\n",
    "n_real = np.sum([int(x==1) for x in nearest_class])\n",
    "n_fake = np.sum([int(x==0) for x in nearest_class])\n",
    "\n",
    "\n",
    "ns = (len(nearest_class))\n",
    "z = 0\n",
    "x = n_fake/len(nearest_class)\n",
    "a = x\n",
    "b = z\n",
    "c = np.sqrt(ns)\n",
    "p_val = (-np.sqrt(-4*a**2*b**2*c**2+4*a*b**2*c**2 + b**4) + 2*a*c**2 + b**2) / (2*(b**2+c**2))\n",
    "\n",
    "nd = len(y)\n",
    "nq = (y==1).sum() \n",
    "data_overlap = -(p_val-1) * (nd / nq)\n",
    "print('data overlap = %f' %data_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d30f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a6352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578b612d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5290d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8efc04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voxel51",
   "language": "python",
   "name": "voxel51"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
